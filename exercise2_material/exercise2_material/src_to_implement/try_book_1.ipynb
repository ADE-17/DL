{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from Layers.Base import BaseLayer\n",
    "\n",
    "class FullyConnected(BaseLayer): # Inherits base layer \n",
    "    \"\"\"\n",
    "    Represents a FullyConnected NN layers, performs forward, backward pass and \n",
    "    updates parameters back to Neural Network\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size, output_size):\n",
    "        # Initialize constructor variables\n",
    "        super().__init__()\n",
    "        self.trainable = True\n",
    "        self.weights = np.random.uniform(0, 1, (input_size + 1, output_size)) # Random weights, +1 for bias column\n",
    "        self._gradient_weights = None\n",
    "        self._gradient_biases = None\n",
    "        self._optimizer = None\n",
    "\n",
    "    def forward(self, input_tensor):\n",
    "        \"\"\"\n",
    "        output_tensor = weights * input_tensor + bias\n",
    "        \"\"\"    \n",
    "        self.input = input_tensor\n",
    "\n",
    "        self.output = np.dot(np.concatenate([input_tensor, np.ones((input_tensor.shape[0], 1))], \n",
    "                                             axis=1), self.weights) \n",
    "        return self.output\n",
    "\n",
    "    def backward(self, error_tensor):\n",
    "        \"\"\"\n",
    "        E_n-1 = weight.T * E_n \n",
    "        weight_t+1 = weight_t - mu * E_n * input_tensor.T\n",
    "        (E_n = error for 'n' layer)\n",
    "        \"\"\"    \n",
    "        self._gradient_weights = np.dot(np.concatenate([self.input, np.ones((self.input.shape[0], 1))],\n",
    "                                                        axis=1).T, error_tensor)\n",
    "        self._gradient_biases = np.sum(error_tensor, axis=0, keepdims=True)\n",
    "        self.gradient_input = np.dot(error_tensor, self.weights[:-1].T)\n",
    "        \n",
    "        if self._optimizer is not None: # Check if optimizer is set for the particular layer\n",
    "            self.weights = self._optimizer.calculate_update(self.weights, self._gradient_weights)\n",
    "\n",
    "        return self.gradient_input\n",
    "\n",
    "    def initialize(self, weights_initializer, bias_initializer):\n",
    "        \"\"\"\n",
    "        Reinitialize the weights using the weights_initializer and biases using bias_initializer.\n",
    "        \"\"\"\n",
    "        self.weights = weights_initializer.initialize(self.weights)\n",
    "\n",
    "        self.weights[:, :-1] = bias_initializer.initialize(self.weights[:, :-1])\n",
    "\n",
    "    # Set getter and setter property for optimizer\n",
    "    @property\n",
    "    def optimizer(self):\n",
    "        return self._optimizer\n",
    "\n",
    "    @optimizer.setter\n",
    "    def optimizer(self, value):\n",
    "        self._optimizer = value\n",
    "\n",
    "    # Set getter and setter property for gradient parameters\n",
    "    @property\n",
    "    def gradient_weights(self):\n",
    "        return self._gradient_weights\n",
    "\n",
    "    @property\n",
    "    def gradient_biases(self):\n",
    "        return self._gradient_biases"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
