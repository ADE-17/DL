{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Layers import FullyConnected, Helpers, Dropout, BatchNormalization, Flatten, RNN, Sigmoid, TanH, Initializers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 9\n",
    "input_size = 13\n",
    "output_size = 5\n",
    "hidden_size = 7\n",
    "input_tensor = np.random.rand(input_size, batch_size).T\n",
    "\n",
    "categories = 4\n",
    "label_tensor = np.zeros([categories, batch_size]).T\n",
    "for i in range(batch_size):\n",
    "    label_tensor[i, np.random.randint(0, categories)] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from Layers.TanH import TanH\n",
    "from Layers.Sigmoid import Sigmoid\n",
    "from Layers.FullyConnected import FullyConnected\n",
    "class RNN:\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "\n",
    "        self.fcy= FullyConnected(self.hidden_size, self.output_size)\n",
    "        self.why = self.fcy.weights # np.random.uniform(size=(self.output_size, self.hidden_size))\n",
    "        self.gradient_why = self.fcy._gradient_weights\n",
    "\n",
    "        self.fcw = FullyConnected ( self.hidden_size + self.input_size, self.hidden_size)\n",
    "        self.weights = self.fcw.weights #np.random.uniform(size=(self.hidden_size + self.input_size + 1, self.hidden_size))\n",
    "        self._gradient_weights= self.fcw._gradient_weights\n",
    "\n",
    "        self.hidden_state = np.zeros((1,hidden_size))\n",
    "        self._memorize = False\n",
    "        self.trainable = True\n",
    "        self._optimizer = None\n",
    "\n",
    "        self.hiddenstates = []\n",
    "        self.outputs = []\n",
    "        self.inputs = []\n",
    "\n",
    "\n",
    "    @property\n",
    "    def memorize(self):\n",
    "        return self._memorize\n",
    "\n",
    "    @memorize.setter\n",
    "    def memorize(self, value):\n",
    "        if isinstance(value, bool):\n",
    "            self._memorize = value\n",
    "        else:\n",
    "            raise ValueError('memorize must be a boolean value.')\n",
    "\n",
    "    @property\n",
    "    def gradient_weights(self):\n",
    "        # return the weights used in the hidden state computation\n",
    "        return self.weights\n",
    "\n",
    "    @gradient_weights.setter\n",
    "    def gradient_weights(self, value):\n",
    "        # set new value to the weights used in the hidden state computation\n",
    "        self.weights = value\n",
    "\n",
    "    def reset_state(self):\n",
    "        # Reset stored inputs, outputs and hidden states to empty lists\n",
    "        self.hiddenstates = []\n",
    "        self.outputs = []\n",
    "        self.inputs = []\n",
    "        # Reset the inputs of fcw and fcy to None\n",
    "        self.fcw.input = None\n",
    "        self.fcy.input = None\n",
    "        # Reset hidden_state to zeros\n",
    "        self.hidden_state = np.zeros((1, self.hidden_size))\n",
    "\n",
    "    def forward(self, input_tensor):\n",
    "        if(not self._memorize):\n",
    "            self.reset_state()\n",
    "        time_steps = input_tensor.shape[0]  # Consider the “batch” dimension as the “time”\n",
    "\n",
    "        output_tensor = np.zeros((time_steps, self.output_size))\n",
    "\n",
    "        for t in range(time_steps):\n",
    "            self.hiddenstates.append(self.hidden_state)\n",
    "            self.inputs.append(input_tensor[t])\n",
    "\n",
    "            x = input_tensor[t].reshape(1,-1)\n",
    "            xs = np.concatenate((self.hidden_state,x), axis=1)\n",
    "            self.fcw.input= xs\n",
    "            self.hidden_state = TanH.forward(self,self.fcw.forward(xs))\n",
    "            self.weights = self.fcw.weights\n",
    "\n",
    "            output_tensor[t] = Sigmoid.forward(self,self.fcy.forward(self.hidden_state))\n",
    "            self.why = self.fcy.weights\n",
    "\n",
    "            self.outputs.append(output_tensor[t])\n",
    "        return output_tensor\n",
    "\n",
    "    def backward(self, error_tensor):\n",
    "        self.time_steps = error_tensor.shape[0]\n",
    "        self.out_error = np.zeros((self.time_steps, self.input_size))\n",
    "\n",
    "        self.gradient_weights_y = np.zeros((self.hidden_size + 1, self.output_size))\n",
    "        self.gradient_weights_h = np.zeros((self.hidden_size+self.input_size+1, self.hidden_size))\n",
    "\n",
    "        count = 0\n",
    "\n",
    "        grad_tanh = 1-self.h_t[1::] ** 2\n",
    "        hidden_error = np.zeros((1, self.hidden_size))\n",
    "        \n",
    "        for b in reversed(range(self.time_steps)):\n",
    "            yh_error = self.FC_y.backward(error_tensor[b][np.newaxis, :])\n",
    "            self.FC_y.input_tensor = np.hstack((self.h_t[b+1], 1))[np.newaxis, :]\n",
    "\n",
    "            grad_yh = hidden_error + yh_error\n",
    "            grad_hidden = grad_tanh[b]*grad_yh\n",
    "            xh_error = self.FC_h.backward(grad_hidden)\n",
    "            hidden_error = xh_error[:, 0:self.hidden_size]\n",
    "            x_error = xh_error[:, self.hidden_size:(self.hidden_size + self.input_size + 1)]\n",
    "            self.out_error[b] = x_error\n",
    "\n",
    "            con = np.hstack((self.h_t[b], self.input_tensor[b],1))\n",
    "            self.FC_h.input_tensor = con[np.newaxis, :]\n",
    "            # print(count, \"\", self.bptt)\n",
    "            if count <= self.bptt:\n",
    "                self.weights_y = self.FC_y.weights\n",
    "                self.weights_h = self.FC_h.weights\n",
    "                self.gradient_weights_y = self.FC_y.gradient_weights\n",
    "                self.gradient_weights_h = self.FC_h.gradient_weights\n",
    "            count += 1\n",
    "\n",
    "        if self.optimizer is not None:\n",
    "            self.weights_y = self.optimizer.calculate_update(self.weights_y, self.gradient_weights_y)\n",
    "            self.weights_h = self.optimizer.calculate_update(self.weights_h, self.gradient_weights_h)\n",
    "            self.FC_y.weights = self.weights_y\n",
    "            self.FC_h.weights = self.weights_h\n",
    "        return self.out_error\n",
    "\n",
    "\n",
    "    def calculate_regularization_loss(self):\n",
    "        # if no regularizer is set, just return 0\n",
    "        if self.optimizer is None or self.optimizer.regularizer is None:\n",
    "            return 0\n",
    "        # otherwise, calculate the regularization loss based on the weights\n",
    "        return self.optimizer.regularizer.norm(self.weights)\n",
    "\n",
    "    def initialize(self, weights_initializer, bias_initializer):\n",
    "        self.fcw.weights = weights_initializer.initialize(self.weights.shape, self.input_size, self.hidden_size)\n",
    "        self.fcy.weights = weights_initializer.initialize(self.why.shape, self.hidden_size, self.output_size)\n",
    "        self.why = self.fcy.weights\n",
    "        self.weights = self.fcw.weights\n",
    "\n",
    "\n",
    "    @property\n",
    "    def optimizer(self):\n",
    "        return self._optimizer\n",
    "\n",
    "    @optimizer.setter\n",
    "    def optimizer(self, optimizer):\n",
    "        self._optimizer = optimizer\n",
    "        self._optimizer.layer = self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'RNN' object has no attribute 'h_t'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m layer \u001b[39m=\u001b[39m RNN(input_size, hidden_size, output_size)\n\u001b[0;32m      2\u001b[0m output_tensor \u001b[39m=\u001b[39m layer\u001b[39m.\u001b[39mforward(input_tensor)\n\u001b[1;32m----> 3\u001b[0m error_tensor \u001b[39m=\u001b[39m layer\u001b[39m.\u001b[39;49mbackward(output_tensor)\n",
      "Cell \u001b[1;32mIn[10], line 93\u001b[0m, in \u001b[0;36mRNN.backward\u001b[1;34m(self, error_tensor)\u001b[0m\n\u001b[0;32m     89\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgradient_weights_h \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mzeros((\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhidden_size\u001b[39m+\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39minput_size\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhidden_size))\n\u001b[0;32m     91\u001b[0m count \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m---> 93\u001b[0m grad_tanh \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\u001b[39m-\u001b[39m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mh_t[\u001b[39m1\u001b[39m::] \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m \u001b[39m2\u001b[39m\n\u001b[0;32m     94\u001b[0m hidden_error \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mzeros((\u001b[39m1\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhidden_size))\n\u001b[0;32m     96\u001b[0m \u001b[39mfor\u001b[39;00m b \u001b[39min\u001b[39;00m \u001b[39mreversed\u001b[39m(\u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtime_steps)):\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'RNN' object has no attribute 'h_t'"
     ]
    }
   ],
   "source": [
    "layer = RNN(input_size, hidden_size, output_size)\n",
    "output_tensor = layer.forward(input_tensor)\n",
    "error_tensor = layer.backward(output_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
